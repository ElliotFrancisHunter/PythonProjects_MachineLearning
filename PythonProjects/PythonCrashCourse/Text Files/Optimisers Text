What are optimisers?

- Optimisers are functions that adjust values within a network to decrease loss(diff between correct and not)
- This happens during training
- Once data points run through the graph, a loss is generated as the difference between the actual and expected answers
- The optimiser's job is to decrease loss by adjusting weights through a process called back propagation
- After adjusting weights, the loss is recalculated during the next pass
- If loss decreases, the process continues, otherwise different weights are adjusted and the process continues


Gradient descent:
- BGD, SGD, MbGD

BGD: batch gradient descent:
- calculates the loss of the entire training data set with each update
- kind of slow as it has to calculate total loss each time
- only use when there is not much complex data

SGD: stochastic gradient descent:
- similar to batch gradient descent but looks at examples one at a time rather than as a whole
- results in high fluctuation and often overshoots but can result in lowering loss more quickly
- flexible and widely used among many different models
- similar to BGD with low learning rate

MbGD: mini-batch gradient descent
- combines BGD and SGD by updating after each mini batch of size 'n'
- can better take advantage of complex optimisation algorithms
- minimises fluctuation without unnecessary computation so convergence optimised



-Momentum algorithms

- Some optimisers take advantage of momentum
- This allows for faster or slower weight adjustment depending on how fast loss is decreasing
- There are many different optimiser algorithms employed throughout various models
-  RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam